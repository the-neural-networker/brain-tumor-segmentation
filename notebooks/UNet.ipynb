{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import nibabel as nib\n",
    "import pydicom as pdm\n",
    "import nilearn as nl\n",
    "import nilearn.plotting as nlplt\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as anim\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "from skimage.util import montage\n",
    "\n",
    "from IPython.display import Image as show_gif\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import Compose, HorizontalFlip\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2 \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(f_path):\n",
    "    img = nib.load(f_path)\n",
    "    img = np.asanyarray(img.dataobj)\n",
    "    img = np.rot90(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape -> (240, 240, 155)\n",
      "mask shape -> (240, 240, 155)\n"
     ]
    }
   ],
   "source": [
    "sample_filename = r'D:\\Masters\\IUB F22\\DLS\\brain-tumor-segmentation\\dataset\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_001\\BraTS20_Training_001_flair.nii'\n",
    "sample_filename2 = r'D:\\Masters\\IUB F22\\DLS\\brain-tumor-segmentation\\dataset\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_001\\BraTS20_Training_001_t1.nii'\n",
    "sample_filename3 = r'D:\\Masters\\IUB F22\\DLS\\brain-tumor-segmentation\\dataset\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_001\\BraTS20_Training_001_t2.nii'\n",
    "sample_filename4 = r'D:\\Masters\\IUB F22\\DLS\\brain-tumor-segmentation\\dataset\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_001\\BraTS20_Training_001_t1ce.nii'\n",
    "\n",
    "sample_filename_mask = r'D:\\Masters\\IUB F22\\DLS\\brain-tumor-segmentation\\dataset\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_001\\BraTS20_Training_001_seg.nii'\n",
    "\n",
    "sample_img = load_image(sample_filename)\n",
    "sample_img2 = load_image(sample_filename2)\n",
    "sample_img3 = load_image(sample_filename3)\n",
    "sample_img4 = load_image(sample_filename4)\n",
    "\n",
    "sample_mask = load_image(sample_filename_mask)\n",
    "\n",
    "print(\"img shape ->\", sample_img.shape)\n",
    "print(\"mask shape ->\", sample_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_WT = sample_mask.copy()\n",
    "mask_WT[mask_WT == 1] = 1\n",
    "mask_WT[mask_WT == 2] = 1\n",
    "mask_WT[mask_WT == 4] = 1\n",
    "\n",
    "mask_TC = sample_mask.copy()\n",
    "mask_TC[mask_TC == 1] = 1\n",
    "mask_TC[mask_TC == 2] = 0\n",
    "mask_TC[mask_TC == 4] = 1\n",
    "\n",
    "mask_ET = sample_mask.copy()\n",
    "mask_ET[mask_ET == 1] = 0\n",
    "mask_ET[mask_ET == 2] = 0\n",
    "mask_ET[mask_ET == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "gs = gridspec.GridSpec(nrows=2, ncols=4, height_ratios=[1, 1.5])\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "flair = ax0.imshow(sample_img[:,:,65], cmap='bone')\n",
    "ax0.set_title(\"FLAIR\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(flair)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "t1 = ax1.imshow(sample_img2[:,:,65], cmap='bone')\n",
    "ax1.set_title(\"T1\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(t1)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "t2 = ax2.imshow(sample_img3[:,:,65], cmap='bone')\n",
    "ax2.set_title(\"T2\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(t2)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax3 = fig.add_subplot(gs[0, 3])\n",
    "t1ce = ax3.imshow(sample_img4[:,:,65], cmap='bone')\n",
    "ax3.set_title(\"T1 contrast\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(t1ce)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax4 = fig.add_subplot(gs[1, 1:3])\n",
    "\n",
    "#ax4.imshow(np.ma.masked_where(mask_WT[:,:,65]== False,  mask_WT[:,:,65]), cmap='summer', alpha=0.6)\n",
    "l1 = ax4.imshow(mask_WT[:,:,65], cmap='summer',)\n",
    "l2 = ax4.imshow(np.ma.masked_where(mask_TC[:,:,65]== False,  mask_TC[:,:,65]), cmap='rainbow', alpha=0.6)\n",
    "l3 = ax4.imshow(np.ma.masked_where(mask_ET[:,:,65] == False, mask_ET[:,:,65]), cmap='winter', alpha=0.6)\n",
    "\n",
    "ax4.set_title(\"\", fontsize=20, weight='bold', y=-0.1)\n",
    "\n",
    "_ = [ax.set_axis_off() for ax in [ax0,ax1,ax2,ax3, ax4]]\n",
    "\n",
    "colors = [im.cmap(im.norm(1)) for im in [l1,l2, l3]]\n",
    "labels = ['Non-Enhancing tumor core', 'Peritumoral Edema ', 'GD-enhancing tumor']\n",
    "patches = [ mpatches.Patch(color=colors[i], label=f\"{labels[i]}\") for i in range(len(labels))]\n",
    "\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4,fontsize = 'xx-large',\n",
    "           title='Mask Labels', title_fontsize=18, edgecolor=\"black\",  facecolor='#c5c6c7')\n",
    "plt.suptitle(\"Multimodal Scans -  Data | Manually-segmented mask - Target\", fontsize=20, weight='bold')\n",
    "\n",
    "fig.savefig(\"data_sample.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "fig.savefig(\"data_sample.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalConfig:\n",
    "    root_dir = '../dataset'\n",
    "    train_root_dir = '../dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "    test_root_dir = '../dataset/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n",
    "    # path_to_csv = './train_data.csv'\n",
    "    # pretrained_model_path = '../input/brats20logs/brats2020logs/unet/last_epoch_model.pth'\n",
    "    # train_logs_path = '../input/brats20logs/brats2020logs/unet/train_log.csv'\n",
    "    # ae_pretrained_model_path = '../input/brats20logs/brats2020logs/ae/autoencoder_best_model.pth'\n",
    "    # tab_data = '../input/brats20logs/brats2020logs/data/df_with_voxel_stats_and_latent_features.csv'\n",
    "    seed = 55\n",
    "    \n",
    "def seed_everything(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "config = GlobalConfig()\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df -> (201, 12) val_df -> (34, 12) test_df -> (133, 10)\n"
     ]
    }
   ],
   "source": [
    "survival_info_df = pd.read_csv('../dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/survival_info.csv')\n",
    "name_mapping_df = pd.read_csv('../dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/name_mapping.csv')\n",
    "\n",
    "name_mapping_df.rename({'BraTS_2020_subject_ID': 'Brats20ID'}, axis=1, inplace=True) \n",
    "\n",
    "\n",
    "df = survival_info_df.merge(name_mapping_df, on=\"Brats20ID\", how=\"right\")\n",
    "\n",
    "paths = []\n",
    "for _, row  in df.iterrows():\n",
    "    \n",
    "    id_ = row['Brats20ID']\n",
    "    phase = id_.split(\"_\")[-2]\n",
    "    \n",
    "    if phase == 'Training':\n",
    "        path = os.path.join(config.train_root_dir, id_)\n",
    "    else:\n",
    "        path = os.path.join(config.test_root_dir, id_)\n",
    "    paths.append(path)\n",
    "    \n",
    "df['path'] = paths\n",
    "\n",
    "#split data on train, test, split\n",
    "#train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=69, shuffle=True)\n",
    "#train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "train_data = df.loc[df['Age'].notnull()].reset_index(drop=True)\n",
    "train_data[\"Age_rank\"] =  train_data[\"Age\"] // 10 * 10\n",
    "train_data = train_data.loc[train_data['Brats20ID'] != 'BraTS20_Training_355'].reset_index(drop=True, )\n",
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=7, random_state=config.seed, shuffle=True\n",
    ")\n",
    "for i, (train_index, val_index) in enumerate(\n",
    "        skf.split(train_data, train_data[\"Age_rank\"])\n",
    "        ):\n",
    "        train_data.loc[val_index, \"fold\"] = i\n",
    "\n",
    "train_df = train_data.loc[train_data['fold'] != 0].reset_index(drop=True)\n",
    "val_df = train_data.loc[train_data['fold'] == 0].reset_index(drop=True)\n",
    "\n",
    "test_df = df.loc[~df['Age'].notnull()].reset_index(drop=True)\n",
    "print(\"train_df ->\", train_df.shape, \"val_df ->\", val_df.shape, \"test_df ->\", test_df.shape)\n",
    "train_data.to_csv(\"train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, phase: str=\"test\", is_resize: bool=False):\n",
    "        self.df = df\n",
    "        self.phase = phase\n",
    "        self.augmentations = get_augmentations(phase)\n",
    "        self.data_types = ['_flair.nii', '_t1.nii', '_t1ce.nii', '_t2.nii']\n",
    "        self.is_resize = is_resize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_ = self.df.loc[idx, 'Brats20ID']\n",
    "        root_path = self.df.loc[self.df['Brats20ID'] == id_]['path'].values[0]\n",
    "        # load all modalities\n",
    "        images = []\n",
    "        for data_type in self.data_types:\n",
    "            img_path = os.path.join(root_path, id_ + data_type)\n",
    "            img = self.load_img(img_path)#.transpose(2, 0, 1)\n",
    "            \n",
    "            if self.is_resize:\n",
    "                img = self.resize(img)\n",
    "    \n",
    "            img = self.normalize(img)\n",
    "            images.append(img)\n",
    "        img = np.stack(images)\n",
    "        img = np.moveaxis(img, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "        if self.phase != \"test\":\n",
    "            mask_path =  os.path.join(root_path, id_ + \"_seg.nii\")\n",
    "            mask = self.load_img(mask_path)\n",
    "            \n",
    "            if self.is_resize:\n",
    "                mask = self.resize(mask)\n",
    "                mask = np.clip(mask.astype(np.uint8), 0, 1).astype(np.float32)\n",
    "                mask = np.clip(mask, 0, 1)\n",
    "            mask = self.preprocess_mask_labels(mask)\n",
    "    \n",
    "            augmented = self.augmentations(image=img.astype(np.float32), \n",
    "                                           mask=mask.astype(np.float32))\n",
    "            \n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            return {\n",
    "                \"Id\": id_,\n",
    "                \"image\": img,\n",
    "                \"mask\": mask,\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"Id\": id_,\n",
    "            \"image\": img,\n",
    "        }\n",
    "    def load_img(self, file_path):\n",
    "        data = nib.load(file_path)\n",
    "        data = np.asarray(data.dataobj)\n",
    "        return data\n",
    "    \n",
    "    def normalize(self, data: np.ndarray):\n",
    "        data_min = np.min(data)\n",
    "        return (data - data_min) / (np.max(data) - data_min)\n",
    "    \n",
    "    def resize(self, data: np.ndarray):\n",
    "        data = resize(data, (78, 120, 120), preserve_range=True)\n",
    "        return data\n",
    "    \n",
    "    def preprocess_mask_labels(self, mask: np.ndarray):\n",
    "\n",
    "        mask_WT = mask.copy()\n",
    "        mask_WT[mask_WT == 1] = 1\n",
    "        mask_WT[mask_WT == 2] = 1\n",
    "        mask_WT[mask_WT == 4] = 1\n",
    "\n",
    "        mask_TC = mask.copy()\n",
    "        mask_TC[mask_TC == 1] = 1\n",
    "        mask_TC[mask_TC == 2] = 0\n",
    "        mask_TC[mask_TC == 4] = 1\n",
    "\n",
    "        mask_ET = mask.copy()\n",
    "        mask_ET[mask_ET == 1] = 0\n",
    "        mask_ET[mask_ET == 2] = 0\n",
    "        mask_ET[mask_ET == 4] = 1\n",
    "\n",
    "        mask = np.stack([mask_WT, mask_TC, mask_ET])\n",
    "        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentations(phase):\n",
    "    list_transforms = []\n",
    "    \n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    path_to_csv: str,\n",
    "    phase: str,\n",
    "    fold: int = 0,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 4,\n",
    "):\n",
    "    '''Returns: dataloader for the model training'''\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "    \n",
    "    train_df = df.loc[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df.loc[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    dataset = dataset(df, phase)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = get_dataloader(dataset=BratsDataset, path_to_csv='train_data.csv', phase='valid', fold=0)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 362064, 382612, 112028, 175276) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32md:\\Masters\\IUB F22\\DLS\\dls_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2288.0_x64__qbz5n2kfra8p0\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_empty\u001b[39m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataloader))\n\u001b[0;32m      2\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape, data[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32md:\\Masters\\IUB F22\\DLS\\dls_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Masters\\IUB F22\\DLS\\dls_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\Masters\\IUB F22\\DLS\\dls_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1315\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1314\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1315\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1316\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1317\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Masters\\IUB F22\\DLS\\dls_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1175\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1178\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 362064, 382612, 112028, 175276) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "data = next(iter(dataloader))\n",
    "data['Id'], data['image'].shape, data['mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img_tensor \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \n\u001b[0;32m      2\u001b[0m mask_tensor \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNum uniq Image values :\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(img_tensor, return_counts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "img_tensor = data['image'].squeeze()[0].cpu().detach().numpy() \n",
    "mask_tensor = data['mask'].squeeze()[0].squeeze().cpu().detach().numpy()\n",
    "print(\"Num uniq Image values :\", len(np.unique(img_tensor, return_counts=True)[0]))\n",
    "print(\"Min/Max Image values:\", img_tensor.min(), img_tensor.max())\n",
    "print(\"Num uniq Mask values:\", np.unique(mask_tensor, return_counts=True))\n",
    "\n",
    "image = np.rot90(montage(img_tensor))\n",
    "mask = np.rot90(montage(mask_tensor)) \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n",
    "ax.imshow(image, cmap ='bone')\n",
    "ax.imshow(np.ma.masked_where(mask == False, mask),\n",
    "           cmap='cool', alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dls_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51c42bd4e61d2dbd7683af0363aff7a77c3d541b0b69ada06519335cb9266f7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
